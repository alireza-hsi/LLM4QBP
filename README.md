# LLM4QBP
Repository for the paper LLM4QBP which contains codes, prompt, experiment results, etc.


This repository provides a unified pipeline for generating, mapping, and evaluating BPMN process models using two frameworks:
- **MAO**
- **ProMoAI**

It supports activity mapping, BPMN similarity comparison, and experiment result logging, and is designed for reproducible, multi-run experiments.

## Quick Start

Welcome to **LLM4QBP**! This repository provides everything you need to generate, map, and evaluate BPMN process models using LLMs. Here’s how to get started:

### The Fastest Way: Run with Docker

You can now run the entire app using Docker (no need to install conda or Python dependencies manually!)

1. **Pull the Docker image**:
   ```sh
   docker pull alirezahsi/llm4qbp:latest
   ```

2. **Run the Streamlit web UI:**
   ```sh
   docker run --rm -p 8501:8501 -v $(pwd):/app alirezahsi/llm4qbp:latest streamlit run streamlit_runner.py
   ```
   - This will open the web UI at [http://localhost:8501](http://localhost:8501).
   - The `-v $(pwd):/app` option mounts your current directory into the container, so you can access your files and save outputs.

3. **Run the pipeline or dashboard from the command line:**
   ```sh
   # Example: Run the pipeline (replace <args> as needed)
   docker run --rm -v $(pwd):/app alirezahsi/llm4qbp:latest python pipeline.py <args>

   # Example: Run the dashboard
   docker run --rm -p 8501:8501 -v $(pwd):/app alirezahsi/llm4qbp:latest streamlit run dashboard.py
   ```

> **Tip:** You can use all features (web UI, command-line pipeline, dashboard) inside Docker. No local Python or conda setup required!

### (Alternative) Run Locally with Conda

- **Explore LLM-generated QBP and Prompt Samples:**  
  Find random QBP (Question-Based Process) samples and prompt examples generated by LLMs in the [`LLM-generated QBP and Prompt Examples`](./LLM-generated%20QBP%20and%20Prompt%20Examples) folder.  
  - Each sample comes with its corresponding `.svg` file for easy visualization.
  - Subfolders are organized by framework (`MAO`, `ProMoAI`) and QBP number.

- **Database of Experiment Runs:**  
  All experiment runs are logged in a detailed SQLite database (default: `resultsDb.sqlite`).  
  - Each row contains information about a single run, including parameters, similarity scores, and file paths.
  - Use the `generated_bpmn_path` column to locate the exact `.bpmn` file generated for any run.

- **Find Your Outputs and Logs:**  
  - All generated BPMN files and run logs are saved in their corresponding subfolders under the `WareHouse` directory of each framework:
    - For MAO: `MAO/Code/WareHouse/`
    - For ProMoAI: `ProMoAI/WareHouse/`
  - Each run gets its own timestamped folder containing the BPMN file and logs.

- **Try It Yourself – Recommended: Streamlit Web UI!**  
  The easiest way to run your own experiments is with our Streamlit web interface.  
  **To launch the web UI, simply run:**
  ```sh
  streamlit run streamlit_runner.py
  ```
  - This will open a browser window where you can upload your QBP task and gold BPMN files, select parameters, and run the pipeline interactively.
  - Results and logs will be saved just as in the command-line version.

- **Ready to Run from the Command Line?**  
  - See the [Usage](#usage) section below for command-line instructions.


---

## Directory Structure

```
├── pipeline.py # Main pipeline script for running experiments
├── streamlit_runner.py # Streamlit web UI for running the pipeline
├── dashboard.py # Dashboard for results visualization
├── dbModifier.py # Utility script for modifying the results database
├── resultsDb.sqlite # Example SQLite results database
├── 15_runs_results.sqlite # Another example results database
├── LLM-generated QBP and Prompt Examples/
│ ├── MAO/
│ │ ├── QBP 1/
│ │ ├── QBP 2/
│ │ ├── QBP 3/
│ │ └── QBP 4/
│ └── ProMoAI/
│ ├── QBP1/
│ ├── QBP2/
│ ├── QBP3/
│ ├── QBP4/
│ └── Prompt example.txt
├── ProMoAI/
│ ├── ProMoAI_API.py
│ ├── WareHouse/ # ProMoAI output BPMN files and logs (per run)
│ ├── QBP_1/ ... QBP_4/ # QBP-specific folders
│ └── ...
├── MAO/
│ └── Code/
│ ├── run.py
│ ├── Helper/
│ ├── WareHouse/ # MAO output BPMN files and logs (per run)
│ └── ...
├── QBPs/ # Example QBP input files
├── WareHouse/ # (Legacy/unused) Top-level output directory
└── README.md
```

---

## Requirements

- Python 3.8+
- [Anaconda/Miniconda](https://docs.conda.io/en/latest/)
- The following conda environments:
  - `base` (for ProMoAI)
  - `MAO_conda_env` (for MAO)
- All dependencies listed in `MAO/Code/requirements.txt` and `ProMoAI/requirements.txt` (if present)

---

## Usage

### 1. **Command-Line Pipeline**

Run the pipeline for either framework:

```sh
# For ProMoAI
conda run -n base python pipeline.py \
  --framework ProMoAI \
  --task-file <input_task.txt> \
  --gold-bpmn <gold.bpmn> \
  --runs 3 \
  --name MyExperiment \
  --results-db my_results.sqlite \
  --model GPT_4o1

# For MAO
conda run -n MAO_conda_env python pipeline.py \
  --framework MAO-v2.2 \
  --task-file <input_task.txt> \
  --gold-bpmn <gold.bpmn> \
  --runs 3 \
  --name MyExperiment \
  --results-db my_results.sqlite \
  --model GPT_4o1 \
  --config Default \
  --org DefaultOrganization
```

### 2. **Streamlit Web UI for Running the Pipeline**

Launch the web interface to run experiments:

```sh
streamlit run streamlit_runner.py
```

- Upload your task and gold BPMN files, select parameters, and run the pipeline interactively.

### 3. **Streamlit Dashboard for Results Visualization**

Visualize and analyze your experiment results with the dashboard:

```sh
streamlit run dashboard.py
```

- This app reads your results database (e.g., `resultsDb.sqlite`) and provides interactive tables and plots for:
  - Node/structure similarity
  - Graph edit distance
  - Framework/model comparisons
  - Filtering and exploring individual runs

## Output

- **Generated BPMN files** are saved in:
  - `ProMoAI/WareHouse/` (for ProMoAI)
  - `MAO/Code/WareHouse/` (for MAO)
- **Experiment results** are logged in the specified SQLite database (default: `resultsDb.sqlite`).

Each run logs:
- Timestamps
- QBP/task name
- Framework and model used
- Node and structure similarity
- Graph edit distance
- Paths to gold and generated BPMN files
- Retry info (for ProMoAI)

---

## Customization

- **Directory Structure:**  
  The pipeline expects the output directories to match the structure above. If you change the structure, update the path handling in `pipeline.py`.

---

## Troubleshooting

- **FileNotFoundError:**  
  Ensure the generated BPMN file paths are correct and that the pipeline is using the full path, not just a relative one.
- **Conda Environment Issues:**  
  Make sure you have the correct environments and dependencies installed.

---

## License



---

## Acknowledgements

- MAO and ProMoAI frameworks
- OpenAI GPT models and APIs
- This project uses [bpmn-auto-layout](https://github.com/bpmn-io/bpmn-auto-layout) for automatic BPMN diagram layout.


---

**For questions or contributions, please open an issue or pull request.**


